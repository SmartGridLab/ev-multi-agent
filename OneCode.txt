(分割した前に上手く行ってた状態のコードです。分割バージョンで問題無かったら消す予定)


# --------------------------
# ハイパーパラメータ定義
# --------------------------
# 共通
NUM_EPISODES = 20000      # 総エピソード数

# RandomForest 用ハイパーパラメータ
RF_TRAIN_INTERVAL = 1000     # RF学習呼び出し間隔（エピソード毎）
RF_N_ESTIMATORS = 10       # RandomForestの決定木の数

# MADDPG 用ハイパーパラメータ
LR_ACTOR = 1e-3            # Actor の学習率
LR_CRITIC = 1e-3           # Critic の学習率
STATE_DIM = 5              # 環境状態の次元
ACTION_DIM = 1             # 行動の次元
NUM_AGENTS = 2             # エージェント数
MADDPG_HIDDEN_SIZE = 64    # ネットワークの隠れ層のサイズ
GAMMA = 0.95               # 割引率
TAU = 0.01                 # ソフトアップデート係数
BATCH_SIZE = 256            # バッチサイズ
MEMORY_SIZE = int(1e6)     # Replay Buffer のサイズ

# EV環境用ハイパーパラメータ
CAPACITY = 100.0           # EVの容量（100に設定）
INITIAL_SOC = 0.0          # 初期 SoC 値
AG_REQUEST = 100.0         # （この値は使用せず初期値はrandom.uniform(0, 5)で決定）
EPISODE_STEPS = 48         # 1エピソード内のステップ数
TOLERANCE = 0.1            # 成功判定用の許容誤差
SEQUENTIAL_MODE = True     # 逐次実行モードの有効/無効
PENALTY_WEIGHT = 100.0     # 充電上限超過時のペナルティ（ー100に相当）

# Ornstein-Uhlenbeck Noise 用ハイパーパラメータ
OU_MU = 0.0                # 平均
OU_THETA = 0.15            # 速度（戻りの速さ）
OU_SIGMA = 0.2             # ノイズの振幅
OU_DT = 1e-2               # 時間刻み

# Critic 初期化用ハイパーパラメータ
CRITIC_INIT_W = 3e-3       # Critic 最終層の重み初期化幅

# --------------------------
# 必要なライブラリのインポート
# --------------------------
import os
import copy
import random
from collections import deque

from matplotlib import font_manager, rcParams
import numpy as np
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.nn.utils import clip_grad_norm_

from sklearn.ensemble import RandomForestRegressor

# 日本語フォント設定（MS ゴシックを使用）
font_path = r"C:\Users\hayas\Desktop\EVMA\MSGOTHIC.TTC"  # MS ゴシックのパス
font_prop = font_manager.FontProperties(fname=font_path)
rcParams['font.family'] = font_prop.get_name()
rcParams['axes.unicode_minus'] = False

# デバイス設定
# デバイス設定（GPUが使えるならGPUを利用）
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
print("Using device:", device)



# --------------------------
# OpponentPredictorRF：ランダムフォレストによる予測モジュール
# --------------------------
class OpponentPredictorRF:
    def __init__(self, n_estimators=RF_N_ESTIMATORS):
        self.model = RandomForestRegressor(n_estimators=n_estimators)
        self.is_trained = False

    def predict(self, X):
        if not self.is_trained:
            return random.uniform(0, 5)  # 未学習時は0～5のランダムな値を返す
        prediction = self.model.predict([X])[0]
        return np.clip(prediction, 0, 2.5)

    def train(self, X, y):
        self.model.fit(X, y)
        self.is_trained = True

rf_predictor_ev1 = OpponentPredictorRF(n_estimators=RF_N_ESTIMATORS)
rf_predictor_ev2 = OpponentPredictorRF(n_estimators=RF_N_ESTIMATORS)

# --------------------------
# EV環境クラス
# --------------------------

class EVEnv:
    def __init__(self, capacity=CAPACITY, initial_soc=INITIAL_SOC, ag_request=None, 
                 episode_steps=48, tolerance=TOLERANCE, sequential_mode=SEQUENTIAL_MODE):
        # 放電は考慮せず、充電のみでSoCが増加。容量はCAPACITYに従う
        self.capacity = capacity  
        self.initial_soc = initial_soc  # 初期SoCは0
        # 初期AG要請は指定がなければ範囲[0,5]から選ぶ
        self.ag_request = random.uniform(0, 5) if ag_request is None else ag_request
        self.episode_steps = episode_steps  # 1エピソードは48ステップ
        self.tolerance = tolerance
        self.sequential_mode = sequential_mode  
        self.reset()
    
    def reset(self):
        self.soc = {"ev1": self.initial_soc, "ev2": self.initial_soc}
        self.step_count = 0
        self.temp_actions = {}
        self.temp_predicted_first = None
        self.temp_predicted_second = None
        self.temp_first_agent = None
        self.prediction_error_sum = 0.0
        # 状態は5次元：[自身のSoC, 相手のSoC, エージェント識別子, AG充電要請, 予測または相手の充電量]
        state_ev1 = self.get_state_for_agent("ev1", other_charge=None)
        state_ev2 = self.get_state_for_agent("ev2", other_charge=None)
        return np.array([state_ev1, state_ev2])
    
    def get_state_for_agent(self, agent, other_charge=None):
        # other_chargeがNoneの場合は予測値（なければ0～5の乱数）を利用
        if other_charge is None:
            if agent == "ev1":
                other_val = self.temp_predicted_second if self.temp_predicted_second is not None else random.uniform(0, 5)
            elif agent == "ev2":
                other_val = self.temp_predicted_first if self.temp_predicted_first is not None else random.uniform(0, 5)
            else:
                raise ValueError("Unknown agent")
        else:
            other_val = other_charge

        if agent == "ev1":
            return np.array([self.soc["ev1"], self.soc["ev2"], 0.0, self.ag_request, other_val], dtype=np.float32)
        elif agent == "ev2":
            return np.array([self.soc["ev2"], self.soc["ev1"], 1.0, self.ag_request, other_val], dtype=np.float32)
        else:
            raise ValueError("Unknown agent")
        
    def get_first_agent(self):
        # ステップカウントにより先行エージェントを交互に決定
        return "ev1" if ((self.step_count + 1) % 2) == 1 else "ev2"
    
    def step(self, actions):
        a1 = actions["ev1"]
        a2 = actions["ev2"]
        
        # EV1の充電更新（CAPACITYを上限とする）
        penalty_ev1 = False
        if self.soc["ev1"] + a1 > self.capacity:
            self.soc["ev1"] = self.capacity
            penalty_ev1 = True
        else:
            self.soc["ev1"] += a1
        
        # EV2の充電更新
        penalty_ev2 = False
        if self.soc["ev2"] + a2 > self.capacity:
            self.soc["ev2"] = self.capacity
            penalty_ev2 = True
        else:
            self.soc["ev2"] += a2
        
        # 現在のAG要請は状態に含まれている値を使う
        total_charge = a1 + a2
        deviation = abs(total_charge - self.ag_request)
        
        # 新しい報酬式（ピースワイズ定義）
        if deviation <= 0.5:
            reward_total = 100
        elif deviation < 1.5:
            reward_total = 20 * (1.5 - deviation)
        else:
            reward_total = 0
        
        reward_ev1 = reward_total
        reward_ev2 = reward_total
        
        # 充電更新で上限を超えた場合の固定ペナルティ（PENALTY_WEIGHTを利用）
        if penalty_ev1:
            reward_ev1 -= PENALTY_WEIGHT
        if penalty_ev2:
            reward_ev2 -= PENALTY_WEIGHT
        
        self.step_count += 1
        
        # 次ステップ用に新たなAG要請を更新（0～5の範囲からrandom.uniformで選ぶ）
        self.ag_request = random.uniform(0, 5)
        
        # 新しいAG要請が反映された状態を生成
        state_ev1 = self.get_state_for_agent("ev1", other_charge=None)
        state_ev2 = self.get_state_for_agent("ev2", other_charge=None)
        next_state = np.array([state_ev1, state_ev2])
        
        rewards = np.array([reward_ev1, reward_ev2], dtype=np.float32)
        done_flags = np.array([self.step_count >= self.episode_steps, self.step_count >= self.episode_steps], dtype=np.float32)
        info = {"total_charge": total_charge, "reward_total": reward_total, "success": (deviation < self.tolerance)}
        return next_state, rewards, done_flags, info

    def step_sequential(self, agent_id, action):
        if not self.temp_actions:
            first_agent = self.get_first_agent()
            if agent_id != first_agent:
                raise ValueError(f"Expected first action from {first_agent}, but got {agent_id}")
            state = self.get_state_for_agent(agent_id, other_charge=self.temp_predicted_first)
            self.temp_actions[agent_id] = action
            self.temp_first_agent = agent_id
            return state, None, False, {"message": "Waiting for second agent action"}
        else:
            expected_second = "ev2" if self.temp_first_agent == "ev1" else "ev1"
            if agent_id != expected_second:
                raise ValueError(f"Expected second action from {expected_second}, but got {agent_id}")
            self.temp_actions[agent_id] = action
            a1 = self.temp_actions.get("ev1", 0.0)
            a2 = self.temp_actions.get("ev2", 0.0)
            if self.temp_first_agent == "ev1":
                true_first_soc = self.soc["ev1"]
            else:
                true_first_soc = self.soc["ev2"]
            
            # 逐次実行時の充電更新（CAPACITYを上限とする）
            penalty_ev1 = False
            if self.soc["ev1"] + a1 > self.capacity:
                self.soc["ev1"] = self.capacity
                penalty_ev1 = True
            else:
                self.soc["ev1"] += a1
            
            penalty_ev2 = False
            if self.soc["ev2"] + a2 > self.capacity:
                self.soc["ev2"] = self.capacity
                penalty_ev2 = True
            else:
                self.soc["ev2"] += a2
            
            total_charge = a1 + a2
            deviation = abs(total_charge - self.ag_request)
            if deviation <= 0.5:
                reward_total = 100
            elif deviation < 1.5:
                reward_total = 20 * (1.5 - deviation)
            else:
                reward_total = 0
            
            reward_ev1 = reward_total / 2.0
            reward_ev2 = reward_total / 2.0
            if penalty_ev1:
                reward_ev1 -= PENALTY_WEIGHT
            if penalty_ev2:
                reward_ev2 -= PENALTY_WEIGHT

            prediction_error = abs(self.temp_predicted_first - true_first_soc)
            self.prediction_error_sum += prediction_error

            self.step_count += 1
            
            # 次ステップ用にAG要請を更新（random.uniformを使用）
            self.ag_request = random.uniform(0, 5)
            
            state_ev1 = self.get_state_for_agent("ev1", other_charge=a2)
            state_ev2 = self.get_state_for_agent("ev2", other_charge=a1)
            next_state = np.array([state_ev1, state_ev2])
            rewards = np.array([reward_ev1, reward_ev2], dtype=np.float32)
            done_flags = np.array([self.step_count >= self.episode_steps, self.step_count >= self.episode_steps], dtype=np.float32)
            info = {
                "total_charge": total_charge,
                "reward_total": reward_total,
                "success": (deviation < self.tolerance),
                "prediction_error_sum": self.prediction_error_sum
            }
            self.temp_actions = {}
            self.temp_predicted_first = None
            self.temp_predicted_second = None
            self.temp_first_agent = None
            return next_state, rewards, done_flags, info

# --------------------------
# Replay Buffer
# --------------------------
class ReplayBuffer:
    def __init__(self, memory_size=MEMORY_SIZE):
        self.memory = deque(maxlen=memory_size)
     
    def cache(self, state, next_state, action, reward, done):
        self.memory.append((state, next_state, action, reward, done))
     
    def sample(self, batch_size=BATCH_SIZE):
        batch = random.sample(self.memory, batch_size)
        state, next_state, action, reward, done = map(np.array, zip(*batch))
        # ここでtorch.tensor(..., device=device)を使って直接GPU上に配置
        return (torch.tensor(state, dtype=torch.float32, device=device),
                torch.tensor(next_state, dtype=torch.float32, device=device),
                torch.tensor(action, dtype=torch.float32, device=device),
                torch.tensor(reward, dtype=torch.float32, device=device),
                torch.tensor(done, dtype=torch.float32, device=device))

# --------------------------
# Actor (Policy Network)
# --------------------------
class Actor(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_size=MADDPG_HIDDEN_SIZE):
        super(Actor, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, action_dim)
    
    def forward(self, state):
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        x = F.softplus(self.fc3(x))
        # 必要に応じて上限を設定する場合は、例えば以下のようにクランプする方法も考えられます。
        # x = torch.clamp(x, max=5)
        return x


# --------------------------
# Critic (Q Network)
# --------------------------
class Critic(nn.Module):
    def __init__(self, state_dim, action_dim, num_agents, hidden_size=MADDPG_HIDDEN_SIZE, init_w=CRITIC_INIT_W):
        super(Critic, self).__init__()
        self.fc1 = nn.Linear(state_dim * num_agents, hidden_size)
        self.fc2 = nn.Linear(hidden_size + action_dim * num_agents, hidden_size)
        self.fc3 = nn.Linear(hidden_size, 1)
        self.fc3.weight.data.uniform_(-init_w, init_w)
        self.fc3.bias.data.uniform_(-init_w, init_w)
    
    def forward(self, states, actions):
        batch_size = states.size(0)
        states_flat = states.view(batch_size, -1)
        actions_flat = actions.view(batch_size, -1)
        x = F.relu(self.fc1(states_flat))
        x = torch.cat([x, actions_flat], dim=1)
        x = F.relu(self.fc2(x))
        q = self.fc3(x)
        return q

# --------------------------
# Ornstein-Uhlenbeck Noise
# --------------------------
class OrnsteinUhlenbeckProcess:
    def __init__(self, size, mu=OU_MU, theta=OU_THETA, sigma=OU_SIGMA, dt=OU_DT):
        self.size = size
        self.mu = mu
        self.theta = theta
        self.sigma = sigma
        self.dt = dt
        self.reset()
    
    def reset(self):
        self.x_prev = np.zeros(self.size)
    
    def sample(self):
        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \
            self.sigma * np.sqrt(self.dt) * np.random.randn(*self.size)
        self.x_prev = x
        return x

# --------------------------
# MADDPG エージェントクラス（2エージェント想定）
# --------------------------
class MADDPG:
    def __init__(self, state_dim=STATE_DIM, action_dim=ACTION_DIM, num_agents=NUM_AGENTS, 
                 hidden_size=MADDPG_HIDDEN_SIZE, gamma=GAMMA, tau=TAU, lr_actor=LR_ACTOR, 
                 lr_critic=LR_CRITIC, batch_size=BATCH_SIZE, memory_size=MEMORY_SIZE):
        self.num_agents = num_agents
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.tau = tau
        self.batch_size = batch_size
        
        # 状態ベクトルは5次元になったので、そのまま5を渡す
        self.actors = [Actor(state_dim, action_dim, hidden_size).to(device) for _ in range(num_agents)]
        self.target_actors = [copy.deepcopy(actor).to(device) for actor in self.actors]
        self.actor_optimizers = [optim.Adam(actor.parameters(), lr=lr_actor) for actor in self.actors]
        
        self.critics = [Critic(state_dim, action_dim, num_agents, hidden_size).to(device) for _ in range(num_agents)]
        self.target_critics = [copy.deepcopy(critic).to(device) for critic in self.critics]
        self.critic_optimizers = [optim.Adam(critic.parameters(), lr=lr_critic) for critic in self.critics]
        
        self.buffer = ReplayBuffer(memory_size)
        self.loss_fn = nn.MSELoss()
        self.noise = [OrnsteinUhlenbeckProcess(size=(action_dim,)) for _ in range(num_agents)]
    
    def get_actions(self, state, add_noise=True, decision_mode=False):
        actions = []
        for i in range(self.num_agents):
            # state[i]をGPU上のテンソルに変換
            state_tensor = torch.tensor(state[i], dtype=torch.float32, device=device).unsqueeze(0)
            a = self.actors[i](state_tensor)
            a = a.detach().cpu().numpy().squeeze()
            if add_noise:
                noise_val = self.noise[i].sample()
                if np.ndim(noise_val) > 0:
                    noise_val = noise_val[0]
                a += noise_val
            actions.append([a])
        return np.array(actions)
            
    def update(self):
        if len(self.buffer.memory) < self.batch_size:
            return
        
        state, next_state, action, reward, done = self.buffer.sample(self.batch_size)
        batch_size = state.shape[0]
        
        # 状態拡張の処理を削除し、そのまま使用
        # state: (batch_size, num_agents, state_dim)
        # next_state: (batch_size, num_agents, state_dim)
        state_aug = state
        next_state_aug = next_state
        
        for agent in range(self.num_agents):
            target_actions = []
            for i in range(self.num_agents):
                a = self.target_actors[i](next_state_aug[:, i, :])
                target_actions.append(a)
            target_actions = torch.stack(target_actions, dim=1)
            
            target_q = self.target_critics[agent](next_state_aug, target_actions)
            r = reward[:, agent].unsqueeze(1)
            d = done[:, agent].unsqueeze(1)
            td_target = r + self.gamma * target_q * (1 - d)
            
            current_q = self.critics[agent](state_aug, action)
            critic_loss = self.loss_fn(current_q, td_target.detach())
            self.critic_optimizers[agent].zero_grad()
            critic_loss.backward()
            clip_grad_norm_(self.critics[agent].parameters(), 0.5)
            self.critic_optimizers[agent].step()
            
            current_actions = []
            for i in range(self.num_agents):
                if i == agent:
                    current_actions.append(self.actors[i](state_aug[:, i, :]))
                else:
                    current_actions.append(self.actors[i](state_aug[:, i, :]).detach())
            current_actions = torch.stack(current_actions, dim=1)
            actor_loss = -self.critics[agent](state_aug, current_actions).mean()
            self.actor_optimizers[agent].zero_grad()
            actor_loss.backward()
            clip_grad_norm_(self.actors[agent].parameters(), 0.5)
            self.actor_optimizers[agent].step()
            
            self.soft_update(self.actors[agent], self.target_actors[agent])
            self.soft_update(self.critics[agent], self.target_critics[agent])
    
    def soft_update(self, net, target_net):
        for param, target_param in zip(net.parameters(), target_net.parameters()):
            target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)

# --------------------------
# 充電割当シミュレーション関数
# --------------------------
def simulate_episode_allocation(agent, env):
    """
    充電割当シミュレーションを行う関数です。
    エージェントは、各ステップで決定モード（add_noise=False, decision_mode=True）でアクションを選択し、
    環境の状態は5次元（自身のSoC、相手のSoC、エージェント識別子、充電要求、相手の充電状態の予測値または実測値）です。
    
    この関数は、エピソード全体の報酬の合計（両エージェントの報酬の総和）を返します。
    """
    # 環境のリセット（内部で、prediction_error_sum なども初期化される）
    state = env.reset()
    
    total_reward = 0  # 累積報酬の初期化
    
    for t in range(env.episode_steps):
        # エージェントからノイズなしでアクションを取得（decision_mode=Trueにより状態がそのまま使われる）
        actions = agent.get_actions(state, add_noise=False, decision_mode=True)
        
        # 環境に対して、各エージェントのアクションを同時に適用して状態を更新し、報酬を受け取る
        state, rewards, done, info = env.step({"ev1": actions[0][0], "ev2": actions[1][0]})
        
        # 各ステップの報酬（両エージェント分）を累積する
        total_reward += np.sum(rewards)
        
        # エピソード終了条件に達している場合はループを抜ける
        if np.all(done):
            break
            
    return total_reward

def plot_episode_data(steps, 
                      predicted_ev1, predicted_ev2, 
                      actual_ev1, actual_ev2, 
                      ag_requests):
    """
    48ステップ分のデータをグラフ化する関数です。
    
    Parameters:
      steps          : 各ステップのインデックス（例：np.arange(1, 49)）
      predicted_ev1  : EV1のRF予測充電量（長さ48の配列）
      predicted_ev2  : EV2のRF予測充電量（長さ48の配列）
      actual_ev1     : EV1の実際の充電量（長さ48の配列）
      actual_ev2     : EV2の実際の充電量（長さ48の配列）
      ag_requests    : AGからの要請量（長さ48の配列）
    """
    # 図全体を2行1列のサブプロットで用意
    fig, axs = plt.subplots(2, 1, figsize=(12, 10), sharex=True)
    
    # --------------------------
    # 上段：RF予測充電量の積み上げ棒グラフ＋AG要請曲線
    # --------------------------
    axs[0].bar(steps, predicted_ev1, label="EV1 Predicted")
    axs[0].bar(steps, predicted_ev2, bottom=predicted_ev1, label="EV2 Predicted")
    axs[0].plot(steps, ag_requests, color='black', marker='o', linewidth=2, label="AG Request")
    axs[0].set_ylabel("Predicted Charge")
    axs[0].set_title("Episode Data: Predicted Charging vs AG Request")
    axs[0].legend()
    
    # --------------------------
    # 下段：実際の充電量の積み上げ棒グラフ＋AG要請曲線
    # --------------------------
    axs[1].bar(steps, actual_ev1, label="EV1 Actual")
    axs[1].bar(steps, actual_ev2, bottom=actual_ev1, label="EV2 Actual")
    axs[1].plot(steps, ag_requests, color='black', marker='o', linewidth=2, label="AG Request")
    axs[1].set_xlabel("Step")
    axs[1].set_ylabel("Actual Charge")
    axs[1].set_title("Episode Data: Actual Charging vs AG Request")
    axs[1].legend()
    
    plt.tight_layout()
    plt.show()


def train_maddpg_sequential(num_episodes=NUM_EPISODES):
    env = EVEnv(capacity=CAPACITY, initial_soc=INITIAL_SOC, ag_request=AG_REQUEST, 
                episode_steps=EPISODE_STEPS, tolerance=TOLERANCE, sequential_mode=SEQUENTIAL_MODE)
    agent = MADDPG(STATE_DIM, ACTION_DIM, num_agents=NUM_AGENTS)
    
    # 各RF予測機の学習データ（特徴量：[相手のSOC, AGからの要請] → ラベル：相手の実際の充電量）
    rf_data_ev1 = []  # EV1のRF：EV2の戦略を学習
    rf_labels_ev1 = []
    rf_data_ev2 = []  # EV2のRF：EV1の戦略を学習
    rf_labels_ev2 = []
    
    episode_rewards = []
    
    for ep in range(num_episodes):
        # RF の学習データリセット：ep==0 の場合はリセットせず、ep>0 かつ RF_TRAIN_INTERVAL の倍数でリセット
        if ep > 0 and (ep % RF_TRAIN_INTERVAL == 0):
            rf_data_ev1.clear()
            rf_labels_ev1.clear()
            rf_data_ev2.clear()
            rf_labels_ev2.clear()
        
        # エピソード毎に、各ステップのデータを記録するリストを初期化（グラフ用）
        predicted_ev1_steps = []
        predicted_ev2_steps = []
        actual_ev1_steps = []
        actual_ev2_steps = []
        ag_requests_steps = []
        
        state = env.reset()
        
        for t in range(env.episode_steps):
            current_request = env.ag_request
            
            # === ステップ1：先行エージェントのRF予測（相手の充電量予測） ===
            first_agent = env.get_first_agent()
            if first_agent == "ev1":
                features_first = [state[0][1], state[0][3]]
                pred_first = rf_predictor_ev1.predict(features_first)
                env.temp_predicted_first = pred_first
                env.temp_first_agent = "ev1"
                state_first = env.get_state_for_agent("ev1", other_charge=pred_first)
                action_first = agent.get_actions(np.array([state_first, state[1]]), add_noise=True)[0][0]
            else:
                features_first = [state[1][1], state[1][3]]
                pred_first = rf_predictor_ev2.predict(features_first)
                env.temp_predicted_first = pred_first
                env.temp_first_agent = "ev2"
                state_first = env.get_state_for_agent("ev2", other_charge=pred_first)
                action_first = agent.get_actions(np.array([state[0], state_first]), add_noise=True)[1][0]
            
            _ = env.step_sequential(env.temp_first_agent, action_first)
            first_agent_used = env.temp_first_agent
            
            # === ステップ2：後攻エージェントのRF予測（相手の充電量予測） ===
            second_agent = "ev2" if first_agent_used == "ev1" else "ev1"
            if second_agent == "ev1":
                features_second = [state[0][1], state[0][3]]
                pred_second = rf_predictor_ev1.predict(features_second)
            else:
                features_second = [state[1][1], state[1][3]]
                pred_second = rf_predictor_ev2.predict(features_second)
            env.temp_predicted_second = pred_second
            
            # === ステップ4：後攻エージェントは実測値を利用して行動決定 ===
            state_second = env.get_state_for_agent(second_agent, other_charge=env.temp_actions.get(first_agent_used, -1.0))
            if second_agent == "ev1":
                action_second = agent.get_actions(np.array([state_second, state[1]]), add_noise=True)[0][0]
            else:
                action_second = agent.get_actions(np.array([state[0], state_second]), add_noise=True)[1][0]
            
            # --- 各エージェントの実際の充電量と RF 予測値の整理 ---
            if first_agent_used == "ev1":
                action_ev1 = action_first
                action_ev2 = action_second
                predicted_ev1_this_step = pred_first
                predicted_ev2_this_step = pred_second
            else:
                action_ev2 = action_first
                action_ev1 = action_second
                predicted_ev2_this_step = pred_first
                predicted_ev1_this_step = pred_second
            
            # 記録（グラフ用）
            predicted_ev1_steps.append(predicted_ev1_this_step)
            predicted_ev2_steps.append(predicted_ev2_this_step)
            actual_ev1_steps.append(action_ev1)
            actual_ev2_steps.append(action_ev2)
            ag_requests_steps.append(current_request)
            
            # RF学習用データの蓄積（先行・後攻両方分）
            rf_data_ev1.append([state[0][1], state[0][3]])
            rf_labels_ev1.append(action_ev2)
            rf_data_ev2.append([state[1][1], state[1][3]])
            rf_labels_ev2.append(action_ev1)
            
            # 後攻エージェントの行動実行（充電更新）
            next_state, rewards, done, info = env.step_sequential(second_agent, action_second)
            
            combined_actions = np.array([[action_ev1], [action_ev2]])
            agent.buffer.cache(state, next_state, combined_actions, rewards, done)
            agent.update()
            
            state = next_state
            if np.all(done):
                break
        
        # エピソード全体の報酬（両エージェント分の最終ステップの報酬合計）を記録
        episode_rewards.append(np.sum(rewards))
        
        # 20エピソード毎に、過去20エピソードの平均報酬を出力する
        if (ep + 1) > 0 and (ep + 1) % 100 == 0:
            avg_reward = np.mean(episode_rewards[-100:])
            print(f"Episode {ep+1}: Average Reward: {avg_reward:.3f}")
        
        # RF学習（ep>0 且つ (ep+1) が RF_TRAIN_INTERVAL の倍数のとき）
        if (ep + 1) > 0 and (ep + 1) % RF_TRAIN_INTERVAL == 0:
            if len(rf_data_ev1) > 0:
                rf_predictor_ev1.train(np.array(rf_data_ev1), np.array(rf_labels_ev1))
            if len(rf_data_ev2) > 0:
                rf_predictor_ev2.train(np.array(rf_data_ev2), np.array(rf_labels_ev2))
    
        # 100エピソード毎に、そのエピソードの48ステップ分のデータをグラフ表示（必要に応じて調整）
        if (ep + 1) % 1000 == 0:
            steps = np.arange(1, env.episode_steps + 1)
            plot_episode_data(steps, 
                              predicted_ev1_steps, predicted_ev2_steps, 
                              actual_ev1_steps, actual_ev2_steps, 
                              ag_requests_steps)
    
    # 最終的なエピソード単位の総報酬グラフ表示（各エピソードの報酬）
    plt.figure(figsize=(10, 5))
    episodes = np.arange(len(episode_rewards))
    plt.plot(episodes, episode_rewards, label="Episode Total Reward")
    plt.xlabel("Episode")
    plt.ylabel("Total Reward")
    plt.title("Episode-wise Total Reward")
    plt.legend()
    plt.tight_layout()
    plt.show()
    
    return agent, env, episode_rewards

# Actor・Criticの生成時にGPUへ転送
actor = Actor(STATE_DIM, ACTION_DIM, MADDPG_HIDDEN_SIZE).to(device)
critic = Critic(STATE_DIM, ACTION_DIM, NUM_AGENTS, MADDPG_HIDDEN_SIZE).to(device)

if __name__ == '__main__':
    agent, env, episode_rewards = train_maddpg_sequential(num_episodes=NUM_EPISODES)
